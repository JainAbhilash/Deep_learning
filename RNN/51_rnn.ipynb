{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a654c96d05dddf55e86e5273a4ad88a0",
     "grade": false,
     "grade_id": "cell-ebf50e86d7f66977",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Exercise 5. Recurrent neural networks\n",
    "\n",
    "## Part 1. Training a translation model one sequence at a time\n",
    "\n",
    "## Learning goals of part 1\n",
    "\n",
    "* to get familiar with recurrent neural networks used for sequential data processing\n",
    "* to get familiar with the sequence-to-sequence model for machine translation\n",
    "\n",
    "You may find it useful to look at this tutorial:\n",
    "* [Translation with a Sequence to Sequence Network and Attention](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_training = True  # Set this flag to True before validation and submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "65e2970339980ef7d85c3754662c4ee8",
     "grade": true,
     "grade_id": "evaluation_settings",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# During evaluation, this cell sets skip_training to True\n",
    "# skip_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "206189bfc07ca0dfd1919de315136e94",
     "grade": false,
     "grade_id": "cell-7cb793aa083203d5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data directory is /coursedata\n"
     ]
    }
   ],
   "source": [
    "# Select data directory\n",
    "import os\n",
    "if os.path.isdir('/coursedata'):\n",
    "    course_data_dir = '/coursedata'\n",
    "elif os.path.isdir('../data'):\n",
    "    course_data_dir = '../data'\n",
    "else:\n",
    "    # Specify course_data_dir on your machine\n",
    "    # course_data_dir = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "print('The data directory is %s' % course_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "43e9553618cbf3a00ab7c90607e62a2f",
     "grade": false,
     "grade_id": "cell-1e9abe1f73cbfc08",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the device for training (use GPU if you have one)\n",
    "#device = torch.device('cuda:0')\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cf7664b5f4dbc9a58c95b639ab60792d",
     "grade": false,
     "grade_id": "cell-ca2b059fa5680b32",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "if skip_training:\n",
    "    # The models are always evaluated on CPU\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "44c93ed43eb96c68ec18abb23c390179",
     "grade": false,
     "grade_id": "cell-8b0f174de81f2ae7",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Data\n",
    "\n",
    "The dataset that we are going to use consists of pairs of sentences in French and English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2f5c730885eb3870c1f65f40ab811f0f",
     "grade": false,
     "grade_id": "cell-f30028f7e1e6e5a1",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from data import TranslationDataset, MAX_LENGTH, SOS_token, EOS_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "93f8a05c0613388e47b59610c691bd87",
     "grade": false,
     "grade_id": "cell-c2ae48a91eff5fd6",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "data_dir = os.path.join(course_data_dir, 'translation_data')\n",
    "trainset = TranslationDataset(path=data_dir, train=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c4a95a026b680f28478494cbd846c86a",
     "grade": false,
     "grade_id": "cell-57d874191ca04b5e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "* `TranslationDataset` supports indexing as required by `torch.utils.data.Dataset`\n",
    "* Sentences are tensors of maximum length `MAX_LENGTH`\n",
    "* Words in a (sentence) tensor are represented as an index (integer) in a language vocabulary\n",
    "* The string representation of a word from the input language can be obtained from index `i` with `dataset.input_lang.index2word[i]`\n",
    "* Similarly for the output language `dataset.output_lang.index2word[j]`\n",
    "\n",
    " Let us look at samples from that dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9fad6e7ba94d6d6d74b11de25164b21a",
     "grade": false,
     "grade_id": "cell-2e435bbbefff50b5",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence: \"tu plaisantes ! EOS\"\n",
      "Sentence as tensor of word indices:\n",
      "tensor([[211],\n",
      "        [865],\n",
      "        [ 17],\n",
      "        [  1]])\n",
      "\n",
      "Output sentence: \"you re joking ! EOS\"\n",
      "Sentence as tensor of word indices:\n",
      "tensor([[130],\n",
      "        [ 78],\n",
      "        [104],\n",
      "        [  9],\n",
      "        [  1]])\n"
     ]
    }
   ],
   "source": [
    "input_sentence, output_sentence = trainset[np.random.choice(len(trainset))]\n",
    "print('Input sentence: \"%s\"' % ' '.join(trainset.input_lang.index2word[i.item()] for i in input_sentence))\n",
    "print('Sentence as tensor of word indices:')\n",
    "print(input_sentence)\n",
    "\n",
    "print('\\nOutput sentence: \"%s\"' % ' '.join(trainset.output_lang.index2word[i.item()] for i in output_sentence))\n",
    "print('Sentence as tensor of word indices:')\n",
    "print(output_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of input-output pairs in the training set:  8682\n"
     ]
    }
   ],
   "source": [
    "print('Number of input-output pairs in the training set: ', len(trainset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "39f417fe0474640245b10bf52a24bf50",
     "grade": false,
     "grade_id": "cell-9c08aee8bc963ee0",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Sequence-to-sequence model for machine translation\n",
    "\n",
    "In this exercise, we are going to build a machine translation system which transforms a sentence in one language into a sentence in another one. The computational graph of the translation model is shown below:\n",
    "\n",
    "<img src=\"seq2seq.png\" width=900 style=\"float: left;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "03f371a827ff4912e180470d7d561f84",
     "grade": false,
     "grade_id": "cell-729c83e14264895f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "We are going to use a simplified model without the dotted connections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "162a59b494b71d0a883f32556b82f728",
     "grade": false,
     "grade_id": "cell-ac0e967c23b19a55",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Encoder\n",
    "\n",
    "The encoder encodes an input sequence $(x_1, x_2, ..., x_T)$ into a single vector $h_T$ using the following recursion:\n",
    "$$\n",
    "  h_{t} = f(h_{t-1}, x_t) \\qquad t = 1, \\ldots, T\n",
    "$$\n",
    "where:\n",
    "* intial state $h_0$ is often chosen arbitrarily (we choose it to be zero)\n",
    "* function $f$ is defined by the type of the RNN cell (in our experiments, we will use [GRU](https://pytorch.org/docs/stable/nn.html#torch.nn.GRU))\n",
    "* $x_t$ is a vector that represents the $t$-th word in the input sentence.\n",
    "\n",
    "A common practice in natural language processing is to _learn_ the word representations $x_t$ (instead of, for example, using one-hot coded vectors). In PyTorch, this is supported by class [Embedding](https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding) which we are going to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2d2e4f0851d58835e743d2c3e49bdd4a",
     "grade": false,
     "grade_id": "cell-b1ea51a1e2b62f87",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "The computational graph of the encoder is shown below:\n",
    "\n",
    "<img src=\"seq2seq_encoder.png\" width=500 style=\"float: left;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fb2df031b89781c33799aae9f042d369",
     "grade": false,
     "grade_id": "cell-1f43b6294c6c0dee",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Let us implement the encoder whose `forward` function can processes _one input sequence at a time_.\n",
    "\n",
    "Your task is to implement the `forward` function which should:\n",
    "* embed the words in the input sequence (convert words' indexes into vectors using `self.embedding`)\n",
    "* perform GRU computations by feeding the embedded words and the given state of the GRU cell (`hidden`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d78ebce4d6a2ae08f4f9a35f9bc2562c",
     "grade": false,
     "grade_id": "cell-0dcf2ba41c33535f",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, dictionary_size, hidden_size):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          dictionary_size (int): Size of dictionary in the source language.\n",
    "          hidden_size (int): Size of the hidden state.\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(dictionary_size, hidden_size)\n",
    "        self.gru = nn.GRU(input_size=hidden_size, hidden_size=hidden_size)\n",
    "\n",
    "    def forward(self, input_seq, hidden):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          input_seq (tensor):  Tensor of words (word indices) of the input sentence. The shape is\n",
    "                               [seq_length, batch_size] with batch_size = 1.\n",
    "          hidden (tensor):    The state of the GRU (shape [1, batch_size, hidden_size] with batch_size=1).\n",
    "\n",
    "        Returns:\n",
    "          output (tensor): Output of the GRU (shape [seq_length, 1, hidden_size]).\n",
    "          hidden (tensor): New state of the GRU (shape [1, batch_size, hidden_size] with batch_size=1).\n",
    "        \"\"\"\n",
    "        batch_size = input_seq.size(1)\n",
    "        assert batch_size == 1, \"Encoder can process only one sequence at a time.\"\n",
    "        # YOUR CODE HERE\n",
    "        embedded = self.embedding(input_seq)\n",
    "        outputs, hidden = self.gru(embedded, hidden)\n",
    "        #raise NotImplementedError()\n",
    "        return outputs, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "91b5cc216c8d1b5b50046123bdc24e62",
     "grade": true,
     "grade_id": "encoder",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shapes seem to be ok.\n"
     ]
    }
   ],
   "source": [
    "# Let's test your code\n",
    "hidden_size = 20\n",
    "test_encoder = Encoder(dictionary_size=10, hidden_size=hidden_size).to(device)\n",
    "\n",
    "hidden = test_encoder.init_hidden()\n",
    "input_seq = torch.tensor([1, 2, 3, 4], device=device).view(4, 1)  # reshape to (seq_length, 1)\n",
    "outputs, hidden = test_encoder.forward(input_seq, hidden)\n",
    "assert outputs.shape == torch.Size([4, 1, hidden_size]), \\\n",
    "    \"Bad shape of outputs: outputs.shape={}, expected={}\".format(outputs.shape, torch.Size([4, 1, hidden_size]))\n",
    "assert hidden.shape == torch.Size([1, 1, hidden_size]), \\\n",
    "    \"Bad shape of outputs: hidden.shape={}, expected={}\".format(hidden.shape, torch.Size([1, 1, hidden_size]))\n",
    "\n",
    "print('The shapes seem to be ok.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9161562834fd4661f93277790042b9b1",
     "grade": false,
     "grade_id": "cell-628fbbf86c3ec3b4",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Decoder\n",
    "\n",
    "The decoder takes as input the representation computed by the encoder and transforms it into a sentence in the target language. The computational graph of the decoder is shown below:\n",
    "\n",
    "<img src=\"seq2seq_decoder.png\" width=500 align=\"top\">\n",
    "\n",
    "Notes:\n",
    "* $z_0$ is the output of the encoder, that is $z_0 = h_5$, thus `hidden_size` of the decoder should be the same as `hidden_size` of the encoder.\n",
    "* $y_{i}$ are the log-probabilities of the words in the output language, the dimensionality of $y_{i}$ is the size of the output (target) dictionary.\n",
    "* $z_{i}$ is mapped to $y_{i}$ using a linear layer followed by `F.log_softmax` (because we use `nn.NLLLoss` loss for training).\n",
    "* Each cell of the decoder is a GRU, it receives as inputs the previous state $z_{i-1}$ and relu of the **embedding** of the previous word. Thus, you need to embed the words of the output language as well. The previous word is taken as the word with the maximum log-probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "acd90b49b6a65457c55ffdf22ee4825a",
     "grade": false,
     "grade_id": "cell-55727601ec7df18a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Note that the decoder outputs a word at every step and the same word is used as the input to the recurrent unit at the next step. At the beginning of decoding, the previous word input is fed with a special word SOS which stands for \"start of a sentence\". During training, we know the target sentence for decoding, therefore we can feed the correct words $y_i$ as inputs to the recurrent unit.\n",
    "\n",
    "There is one thing that it is wise to take care of. When the target sentence is fed to the decoder during training, the decoder learns to generate only the next word (this scenario is called \"teacher forcing\" in the literature). In test time, the decoder works differently: It generates the whole sequence using its own predictions as inputs at each step. Therefore, it makes sense to train the decoder to produce full sentences. In order to do that, we will alternate between two modes during training:\n",
    "* \"teacher forcing\": the decoder is fed with the words in the target sequence\n",
    "* no \"teacher forcing\": the decoder generates the output sequence using its own predictions. We will limit the maximum length of generated sequences to `MAX_LENGTH`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9a13b3f2c7cc163d0bae528745542821",
     "grade": false,
     "grade_id": "cell-fe46cc17f3da6d25",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "In the code below, your task is to implement the decoder which has the structure shown in the figure above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5c26d8dc563732df510e530a3ff8d499",
     "grade": false,
     "grade_id": "cell-6c949ebc9aee9ce8",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_dictionary_size):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          hidden_size (int): Size of the hidden state.\n",
    "          output_dictionary_size (int): Size of dictionary in the target language.\n",
    "        \"\"\"\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        self.embedding = nn.Embedding(output_dictionary_size, hidden_size)\n",
    "        self.gru = nn.GRU(input_size=hidden_size, hidden_size=hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_dictionary_size)\n",
    "        #raise NotImplementedError()\n",
    "\n",
    "    def forward(self, hidden, target_seq=None, teacher_forcing=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          hidden (tensor):        The state of the GRU (shape [1, batch_size, hidden_size] with batch_size=1).\n",
    "          target_seq (tensor):    Tensor of words (word indices) of the target sentence. The shape is\n",
    "                                   [target_seq_length, batch_size] with batch_size=1. If None, the output sequence\n",
    "                                   is generated by feeding the decoder's outputs (teacher_forcing has to be False).\n",
    "          teacher_forcing (bool): Whether to use teacher forcing or not.\n",
    "\n",
    "        Returns:\n",
    "          outputs (tensor): Tensor of log-probabilities of words in the output language\n",
    "                             (shape [output_seq_length, batch_size, output_dictionary_size] with batch_size=1).\n",
    "          hidden (tensor):  New state of the GRU (shape [1, batch_size, hidden_size] with batch_size=1).\n",
    "        \"\"\"\n",
    "        if target_seq is None:\n",
    "            assert not teacher_forcing, 'Cannot use teacher forcing without a target sequence.'\n",
    "\n",
    "        prev_word = torch.tensor([SOS_token], device=device, dtype=torch.int64)\n",
    "        out_length = target_seq.size(0) if target_seq is not None else MAX_LENGTH\n",
    "        outputs = []  # Collect decoder outputs at different processing steps in this list\n",
    "        for t in range(out_length):\n",
    "            # YOUR CODE HERE\n",
    "            output=F.relu(self.embedding(prev_word).view(1,1,-1))\n",
    "            output, hidden = self.gru(output, hidden)\n",
    "            output=F.log_softmax(self.out(output),dim=2)\n",
    "            outputs.append(output)\n",
    "            #raise NotImplementedError()\n",
    "\n",
    "            if teacher_forcing:\n",
    "                # Feed the target as the next input\n",
    "                prev_word = target_seq[t]  # Teacher forcing\n",
    "            else:\n",
    "                # Use its own predictions as the next input\n",
    "                topv, topi = output[0, :].topk(1)\n",
    "                prev_word = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "                if prev_word.item() == EOS_token:\n",
    "                    break\n",
    "\n",
    "        outputs = torch.cat(outputs, dim=0)  # [max_length, batch_size, output_dictionary_size]\n",
    "\n",
    "        return outputs, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7d0e9b6fc7da4b43dc85f979d8312ddb",
     "grade": true,
     "grade_id": "decoder",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shapes seem to be ok.\n"
     ]
    }
   ],
   "source": [
    "# Let's test the shapes\n",
    "hidden_size = 20\n",
    "output_dictionary_size = 10\n",
    "test_decoder = Decoder(hidden_size, output_dictionary_size).to(device)\n",
    "\n",
    "hidden = test_decoder.init_hidden()\n",
    "target_seq = torch.tensor([1, 2, 3, 4], device=device).view(4, 1)  # reshape to (seq_length, 1)\n",
    "\n",
    "outputs, hidden = test_decoder.forward(hidden, target_seq, teacher_forcing=False)\n",
    "assert outputs.size(0) <= 4, \"Too long output sequence: outputs.size(0)={}\".format(outputs.size(0))\n",
    "assert outputs.shape[1:] == torch.Size([1, output_dictionary_size]), \\\n",
    "    \"Bad shape of outputs: outputs.shape[1:]={}, expected={}\".format(outputs.shape[1:], torch.Size([1, output_dictionary_size]))\n",
    "assert hidden.shape == torch.Size([1, 1, hidden_size]), \\\n",
    "    \"Bad shape of hidden: hidden.shape={}, expected={}\".format(hidden.shape, torch.Size([1, 1, hidden_size]))\n",
    "\n",
    "outputs, hidden = test_decoder.forward(hidden, target_seq, teacher_forcing=True)\n",
    "assert outputs.shape == torch.Size([4, 1, output_dictionary_size]), \\\n",
    "    \"Bad shape of outputs: outputs.shape={}, expected={}\".format(outputs.shape, torch.Size([4, 1, output_dictionary_size]))\n",
    "assert hidden.shape == torch.Size([1, 1, hidden_size]), \\\n",
    "    \"Bad shape of hidden: hidden.shape={}, expected={}\".format(hidden.shape, torch.Size([1, 1, hidden_size]))\n",
    "\n",
    "# Generation mode\n",
    "outputs, hidden = test_decoder.forward(hidden, target_seq=None, teacher_forcing=False)\n",
    "assert outputs.shape[1:] == torch.Size([1, output_dictionary_size]), \\\n",
    "    \"Bad shape of outputs: outputs.shape[1:]={}, expected={}\".format(outputs.shape[1:], torch.Size([1, output_dictionary_size]))\n",
    "assert hidden.shape == torch.Size([1, 1, hidden_size]), \\\n",
    "    \"Bad shape of hidden: hidden.shape={}, expected={}\".format(hidden.shape, torch.Size([1, 1, hidden_size]))\n",
    "\n",
    "print('The shapes seem to be ok.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "31dc90ab114bfdf0ca83e37d7f89213b",
     "grade": false,
     "grade_id": "cell-6b7d4217c687a659",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Training a sequence-to-sequence model\n",
    "\n",
    "Now we are going to train the sequence-to-sequence model on the toy translation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "842e10bc47437104f00ae6358b0a353b",
     "grade": false,
     "grade_id": "cell-dabbdfdf8108cf41",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Create encoder and decoder\n",
    "hidden_size = 256\n",
    "encoder = Encoder(trainset.input_lang.n_words, hidden_size).to(device)\n",
    "decoder = Decoder(hidden_size, trainset.output_lang.n_words).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3158e3124959b9290f71026f485fc01f",
     "grade": false,
     "grade_id": "cell-a060ee7c89f279ce",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "encoder_optimizer = optim.SGD(encoder.parameters(), lr=0.01)\n",
    "decoder_optimizer = optim.SGD(decoder.parameters(), lr=0.01)\n",
    "criterion = nn.NLLLoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9c97f8706c7a6f1b736a5326685edea0",
     "grade": false,
     "grade_id": "cell-31d7cbf0e0f65325",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6e5c1af6a27ba218e0b3f5fe7ebf85e6",
     "grade": false,
     "grade_id": "cell-a37a0622e06557c5",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "In the training loop below, we are going to process one pair of sequences at a time. Your task is to implement the input sequence encoding and decoding. Toggle `teacher_forcing` on and off during decoding according to the `teacher_forcing_ratio` specified above.\n",
    "\n",
    "The loss computations are implemented already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2bc30cd81c998085118fd489d8424f0a",
     "grade": false,
     "grade_id": "cell-4d30459b69a5b895",
     "locked": false,
     "schema_version": 3,
     "solution": true
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    running_loss = 0.0\n",
    "    print_every = 100  # pairs\n",
    "    for i, (input_seq, target_seq) in enumerate(trainloader):\n",
    "        # We process one sequence at a time\n",
    "        input_seq, target_seq = input_seq[0], target_seq[0]\n",
    "        input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
    "        \n",
    "        encoder_hidden = encoder.init_hidden()\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "\n",
    "        #input_length = input_seq.size(0)\n",
    "        target_length = target_seq.size(0)\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        teacher_forcing = random.random() <= teacher_forcing_ratio\n",
    "        outputs, hidden =encoder.forward(input_seq,encoder_hidden)\n",
    "        \n",
    "        decoder_outputs,decoder_hidden=decoder.forward(hidden,target_seq,teacher_forcing)\n",
    "        #raise NotImplementedError()\n",
    "       \n",
    "        # Compute the loss\n",
    "        # In case of no teacher forcing, the output sequence can be shorter than the target sequence\n",
    "        # We need to take care of that\n",
    "        output_length, _, output_dictionary_size = decoder_outputs.size()\n",
    "        assert (output_length == target_length) or not teacher_forcing, \\\n",
    "            \"In case of teacher forcing, output_length ({}) should be equal to target_length ({}).\".format(\n",
    "            output_length, target_length)\n",
    "        loss = criterion(decoder_outputs.view(output_length, output_dictionary_size),\n",
    "                         target_seq[:output_length].view(output_length))\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item() / target_length\n",
    "        if (i % print_every) == (print_every-1):\n",
    "            print('[%d, %5d] loss: %.4f' % (epoch+1, i+1, running_loss/print_every))\n",
    "            running_loss = 0.0\n",
    "        \n",
    "        if skip_training:\n",
    "            break\n",
    "    if skip_training:\n",
    "        break\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bb50b111dd8ead75ed87632cfb7872f7",
     "grade": false,
     "grade_id": "cell-b3fe533e5e20ec47",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "If you do well, the running loss should reach 0.5-0.6.\n",
    "\n",
    "Hint: The training procedure may take an hour on your laptop. You may first train the model for a few epochs, proceed to the following task and train the model longer later again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3722b242b6cd6eca203b4b6a81b5fb63",
     "grade": true,
     "grade_id": "accuracy",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder loaded from 5_encoder.pth.\n",
      "Decoder loaded from 5_decoder.pth.\n"
     ]
    }
   ],
   "source": [
    "# Save the model to disk, submit these files together with your notebook\n",
    "encoder_filename = '5_encoder.pth'\n",
    "decoder_filename = '5_decoder.pth'\n",
    "if not skip_training:\n",
    "    try:\n",
    "        do_save = input('Do you want to save the model (type yes to confirm)? ').lower()\n",
    "        if do_save == 'yes':\n",
    "            torch.save(encoder.state_dict(), encoder_filename)\n",
    "            torch.save(decoder.state_dict(), decoder_filename)\n",
    "            print('Model saved to %s, %s.' % (encoder_filename, decoder_filename))\n",
    "        else:\n",
    "            print('Model not saved.')\n",
    "    except:\n",
    "        raise Exception('The notebook should be run or validated with skip_training=True.')\n",
    "else:\n",
    "    hidden_size = 256\n",
    "    encoder = Encoder(trainset.input_lang.n_words, hidden_size)\n",
    "    encoder.load_state_dict(torch.load(encoder_filename, map_location=lambda storage, loc: storage))\n",
    "    print('Encoder loaded from %s.' % encoder_filename)\n",
    "    encoder = encoder.to(device)\n",
    "    encoder.eval()\n",
    "\n",
    "    decoder = Decoder(hidden_size, trainset.output_lang.n_words)\n",
    "    decoder.load_state_dict(torch.load(decoder_filename, map_location=lambda storage, loc: storage))\n",
    "    print('Decoder loaded from %s.' % decoder_filename)\n",
    "    decoder = decoder.to(device)\n",
    "    decoder.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "67c82bbbe0d27b3aaaf6246cd11852fb",
     "grade": false,
     "grade_id": "cell-f25db1bf0d47501e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Evaluate the trained model\n",
    "\n",
    "Let us now test the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "77fc408ffc9d46b1da791870cf5df212",
     "grade": false,
     "grade_id": "cell-03281edb32b8c545",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Load the test set\n",
    "testset = TranslationDataset(path=data_dir, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "00fe50d6c1e60cb04cd2399dc3535352",
     "grade": false,
     "grade_id": "cell-d59c5c7d6ea9f648",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Your task is to write a function that takes an input sequence (which is a tensor of word indexes) and produces a sequence of outputs using the trained encoder and decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2af7fbbc53fdb0a3e736a5047bb57490",
     "grade": false,
     "grade_id": "cell-5ee490b93fd0517e",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, input_seq):\n",
    "    \"\"\"Translate given sentence input_seq using trained encoder and decoder.\n",
    "    \n",
    "    Args:\n",
    "      encoder (Encoder): Trained encoder.\n",
    "      decoder (Decoder): Trained decoder.\n",
    "      input_seq (tensor): Tensor of words (word indices) of the input sentence (shape [input_seq_length, 1]).\n",
    "    \n",
    "    Returns:\n",
    "      output_seq (tensor): Tensor of words (word indices) of the output sentence (shape [output_seq_length, 1]).\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "    outputs,hidden=encoder.forward(input_seq,encoder_hidden)\n",
    "    outputs,hidden=decoder.forward(hidden)\n",
    "    value,indices=outputs.topk(1,dim=2)\n",
    "    return indices.view(-1,1) \n",
    "    #raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "013b7c211c7917c2b2c30a1c3dfdb183",
     "grade": true,
     "grade_id": "cell-ac510b081dc92db2",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shapes seem to be ok.\n"
     ]
    }
   ],
   "source": [
    "input_seq = torch.tensor([1, 2, 3, 4], device=device).view(4, 1)  # reshape to (seq_length, 1)\n",
    "output_seq = evaluate(encoder, decoder, input_seq)\n",
    "assert output_seq.shape[0] <= MAX_LENGTH, \\\n",
    "    \"Too long output sequence: output_seq.shape[0]={}\".format(output_seq.shape[0])\n",
    "assert output_seq.shape[1:] == torch.Size([1]), \\\n",
    "    \"Bad shape of output_seq: output_seq.shape[1:]={}, expected={}\".format(output_seq.shape[1:], torch.Size([1]))\n",
    "print('The shapes seem to be ok.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3e2b25552c2b42913bd8bf77286fd69e",
     "grade": false,
     "grade_id": "cell-85766e0f0efdf0de",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Let us now evaluate random sentences from the training set and print the input, target, and output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bf5e7330e401a11718155f7b2658a95c",
     "grade": false,
     "grade_id": "cell-267aa3555353e07b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> elle n admet pas son erreur . EOS\n",
      "= she s not admitting her mistake . EOS\n",
      "< she s not her her mistake . EOS\n",
      "\n",
      "> je commence a apercevoir un motif . EOS\n",
      "= i m beginning to see a pattern . EOS\n",
      "< i m beginning to see a . EOS\n",
      "\n",
      "> tu n es pas tres amusant . EOS\n",
      "= you re not very funny . EOS\n",
      "< you re not very funny . EOS\n",
      "\n",
      "> c est moi la patronne par ici . EOS\n",
      "= i m the boss around here . EOS\n",
      "< i m the boss around here . EOS\n",
      "\n",
      "> je me trouve juste derriere lui . EOS\n",
      "= i m right behind him . EOS\n",
      "< i m right behind him . EOS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    input_sentence, target_sentence = trainset[np.random.choice(len(trainset))]\n",
    "    print('>', ' '.join(trainset.input_lang.index2word[i.item()] for i in input_sentence))\n",
    "    print('=', ' '.join(trainset.output_lang.index2word[i.item()] for i in target_sentence))\n",
    "    output_sentence = evaluate(encoder, decoder, input_sentence.to(device)).view(-1).cpu().data.numpy()\n",
    "    print('<', ' '.join(trainset.output_lang.index2word[i] for i in output_sentence))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "050519a0f219b0102de96b045b9b63b3",
     "grade": false,
     "grade_id": "cell-b28f50fa98ea740b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "If you trained the model well enough, the model should memorize the training data well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3abd187ed77be0b776857be01f416db3",
     "grade": false,
     "grade_id": "cell-1142a3809e449931",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> tu attends trop d elle . EOS\n",
      "= you are expecting too much of her . EOS\n",
      "< you are waiting for her . EOS\n",
      "\n",
      "> je ne suis plus fatigue . EOS\n",
      "= i m no longer tired . EOS\n",
      "< i m no longer tired . EOS\n",
      "\n",
      "> il n est pas encore la . EOS\n",
      "= he s not here yet . EOS\n",
      "< he is not here yet . EOS\n",
      "\n",
      "> vous n etes pas encore adolescente . EOS\n",
      "= you re not a teenager yet . EOS\n",
      "< you re not dead yet . EOS\n",
      "\n",
      "> nous sommes toujours prudentes . EOS\n",
      "= we re always careful . EOS\n",
      "< we re still alive . EOS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate random sentences from the test set and print the input, target, and output\n",
    "for i in range(5):\n",
    "    input_sentence, target_sentence = testset[np.random.choice(len(testset))]\n",
    "    print('>', ' '.join(testset.input_lang.index2word[i.item()] for i in input_sentence))\n",
    "    print('=', ' '.join(testset.output_lang.index2word[i.item()] for i in target_sentence))\n",
    "    output_sentence = evaluate(encoder, decoder, input_sentence.to(device)).view(-1).cpu().data.numpy()\n",
    "    print('<', ' '.join(testset.output_lang.index2word[i] for i in output_sentence))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0c55adbcdfea0fbc38fd75f7dd1889d6",
     "grade": false,
     "grade_id": "cell-c30d227d3c34e44b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "A well-trained model should output sentences that look similar to the target ones. The mistakes are usually done for words that were rare in the training set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
